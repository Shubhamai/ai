// einops, llm.c, ggml, micrograd, tinygrad, candle, gpt2-np, 
// https://github.com/waveworks-ai/fl
// https://www.reddit.com/r/MachineLearning/comments/1102t34/d_have_their_been_any_attempts_to_create_a/
// https://arogozhnikov.github.io/2023/12/28/fastest-autograd.html
// https://huyenchip.com/2021/09/07/a-friendly-introduction-to-machine-learning-compilers-and-optimizers.html

// from micrograd

// everything is a tensor and autograd tracks it
// built-in support of einops, and common layers (relu, attention, linear...)
// with syntax highlighting, online playground, ide integration, repl

// ast optimizer using gpt/llama perhaps ? ( experiment )
// Q: optimize this ast and only return the answer and nothing else `(. x (f (> 1 2) (!= 3 3)))`
// AI: (. x (f false false))



// fun addPair(a, b) {
//   return a + b;
// }

// fun identity(a) {
//   return a;
// }

// print identity(addPair)(1, 2); // Prints "3".



model MyNet (None, 784) {
    
    gelu (None, 1) 0.5 * x * (1 + tanh(sqrt(2 / pi) * (x + 0.044715 * x**3))) -> (None, 1)

    softmax  -> (None, 10) {
        exp(x - max(x, axis=-1, keepdims=True)) / sum(exp(x, axis=-1, keepdims=True))
    } -> (None, 10)

    layer_norm {
        g * (x - mean(x, axis=-1, keepdims=True)) / sqrt(var(x, axis=-1, keepdims=True) + eps) + b
    }

    linear {
        x @ w + b
    }

    ffn {
        gelu(linear(x, **c_fc)) -> x
        linear(x, **c_proj)
    }

    attention {
        softmax(q @ k.T / sqrt(q.shape[-1]) + mask) @ v
    }

    mha {
        linear(x, **c_attn) -> x
        split(x, 3, axis=-1) -> qkv
        map(lambda x: split(x, n_head, axis=-1), qkv) -> qkv_heads
        (1 - tri(x.shape[0], dtype=x.dtype)) * -1e10 -> causal_mask
        [attention(q, k, v, causal_mask) for q, k, v in zip(*qkv_heads)] -> out_heads
        hstack(out_heads) -> x
        linear(x, **c_proj)
    }

    transformer_block {
        layer_norm(x, **ln_1) -> x
        mha(x, **attn, n_head=n_head) -> x
        x + x -> x
        layer_norm(x, **ln_2) -> x
        ffn(x, **mlp) -> x
        x + x
    }

    gpt2 {
        wte[inputs] + wpe[range(len(inputs))] -> x
        for block in blocks {
            transformer_block(x, **block, n_head=n_head) -> x
        }
        layer_norm(x, **ln_f) @ wte.T
    }

    generate {
        for _ in range(n_tokens_to_generate) {
            gpt2(inputs, **params, n_head=n_head) -> logits
            argmax(logits[-1]) -> next_id
            append(inputs, int(next_id))
        }
        inputs[len(inputs) - n_tokens_to_generate :]
    }

    main {
        load_encoder_hparams_and_params(model_size, models_dir) -> encoder, hparams, params
        encoder.encode(prompt) -> input_ids
        assert len(input_ids) + n_tokens_to_generate < hparams["n_ctx"]
        generate(input_ids, params, hparams["n_head"], n_tokens_to_generate) -> output_ids
        encoder.decode(output_ids)
    }



    loss = CrossEntropy(output, labels)
    optimizer = Adam(0.001)
}


# Create the model
net = MyNet()

# Load data
data = Dataset("data.csv")
train, test = data.split(0.8)

# Train
for epoch in 1..10 {
    for batch in train.batches(32) {
        x, y = batch
        
        auto_grad {
            pred = net(x)
            loss = net.loss(pred, y)
        }
        
        net.update(gradients)
    }
    
    accuracy = evaluate(net, test)
    print("Epoch {epoch}: Accuracy = {accuracy}")
}

# Save
net.save("model.bin")